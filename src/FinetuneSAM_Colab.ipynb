{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune SAM Model\n",
    "\n",
    "**Dataset**\n",
    "\n",
    "https://drive.google.com/drive/folders/1na6mkrFLiZZ6l0d4pKXqIsxAUxKhJDAu?usp=drive_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "import os\n",
    "import numpy as np\n",
    "import tifffile\n",
    "from random import randint\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "label_url = 'https://drive.google.com/file/d/1T8RDNBtxuBidm9ttNW9ShauDB49dBjWH/view?usp=drive_link'\n",
    "train_url = 'https://drive.google.com/file/d/1De6cOV0UtS310-vkILWpmY7hiJZRSU9Y/view?usp=drive_link'\n",
    "val_url = 'https://drive.google.com/file/d/1MFLm_5c0G6CUGNx2o2wrwGAKZHvUBCTI/view?usp=drive_link'\n",
    "DVRPC_train_url = 'https://drive.google.com/file/d/1pHzGmjQUvrH1TY4XL1vw8xg72u8K5BuI/view?usp=drive_link'\n",
    "DVRPC_val_url = 'https://drive.google.com/file/d/1YC5oUmGDa0sO14Qc4d-PM8cn2dbU1BKK/view?usp=drive_link'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and unzip the files\n",
    "data_path = os.path.join(os.getcwd(), 'data')\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "label_path = os.path.join(data_path, 'label.tar.gz')\n",
    "train_path = os.path.join(data_path,'train.tar.gz')\n",
    "val_path = os.path.join(data_path,'val.tar.gz')\n",
    "DVRPC_train_path = os.path.join(data_path,'DVRPC_train.json')\n",
    "DVRPC_val_path = os.path.join(data_path,'DVRPC_val.json')\n",
    "\n",
    "train_path_new = os.path.join(data_path, 'Train')\n",
    "if not os.path.exists(train_path_new):\n",
    "    gdown.download(train_url, train_path, fuzzy=True)\n",
    "    !tar -xzf {train_path} -C {data_path}\n",
    "    !rm -rf {train_path}\n",
    "train_path = train_path_new\n",
    "label_path_new = os.path.join(data_path, 'Label')\n",
    "if not os.path.exists(label_path_new):\n",
    "    gdown.download(label_url, label_path, fuzzy=True)\n",
    "    !tar -xzf {label_path} -C {data_path}\n",
    "    # File too large, need to delete the file after unzipping\n",
    "    !rm -rf {label_path} {os.path.join(label_path_new, 'Test2')}\n",
    "label_path = label_path_new\n",
    "val_path_new = os.path.join(data_path, 'Test')\n",
    "if not os.path.exists(val_path_new):\n",
    "    gdown.download(val_url, val_path, fuzzy=True)\n",
    "    !tar -xzf {val_path} -C {data_path}\n",
    "    !rm -rf {val_path}\n",
    "val_path = val_path_new\n",
    "if not os.path.exists(DVRPC_train_path):\n",
    "    gdown.download(DVRPC_train_url, DVRPC_train_path, fuzzy=True)\n",
    "if not os.path.exists(DVRPC_val_path):\n",
    "    gdown.download(DVRPC_val_url, DVRPC_val_path, fuzzy=True)\n",
    "\n",
    "train_label_path = os.path.join(label_path, 'Train')\n",
    "val_label_path = os.path.join(label_path, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = [f for f in os.listdir(train_path) if (f.endswith('.tif') and np.max(tifffile.imread(os.path.join(train_label_path, f))) > 0)]\n",
    "val_files = [f for f in os.listdir(val_path) if (f.endswith('.tif') and np.max(tifffile.imread(os.path.join(val_label_path, f))) > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process json files to get the bounding boxes of an image\n",
    "def preprocess_json(json_file: str):\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    filename_id_map = {image['file_name']: image['id'] for image in data['images']}\n",
    "    ann_map = {}\n",
    "    for ann in data['annotations']:\n",
    "        if ann['image_id'] not in ann_map:\n",
    "            ann_map[ann['image_id']] = []\n",
    "        ann_map[ann['image_id']].append(ann['bbox'])\n",
    "    return filename_id_map, ann_map\n",
    "\n",
    "def filename2bbox(filename: str, filename_id_map: dict, ann_map: dict):\n",
    "    image_id = filename_id_map[filename]\n",
    "    if image_id not in ann_map:\n",
    "        return None\n",
    "    return ann_map[image_id]\n",
    "\n",
    "train_filename_id_map, train_ann_map = preprocess_json(DVRPC_train_path)\n",
    "val_filename_id_map, val_ann_map = preprocess_json(DVRPC_val_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "%matplotlib inline\n",
    "index = randint(0, len(train_files)-1)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "img = tifffile.imread(os.path.join(train_path, train_files[index]))\n",
    "label = tifffile.imread(os.path.join(train_label_path, train_files[index]))\n",
    "ax[0].imshow(img)\n",
    "ax[0].set_title('Image')\n",
    "ax[1].imshow(label * 255, cmap='gray')\n",
    "ax[1].set_title('Mask')\n",
    "plt.show()\n",
    "\n",
    "print('bboxes: ', filename2bbox(train_files[index], train_filename_id_map, train_ann_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required libraries\n",
    "# Transformers\n",
    "%pip install -q transformers\n",
    "# monai if you want to use special loss functions\n",
    "%pip install -q monai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import SamModel, SamProcessor\n",
    "from torch.optim import Adam\n",
    "from monai.losses import DiceLoss\n",
    "from tqdm import tqdm\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SidewalkDataset(Dataset):\n",
    "    def __init__(self, data_path: str, label_path: str, filename_id_map: dict, ann_map: dict, files: list, processor, transform=None):\n",
    "        self.data_path = data_path\n",
    "        self.label_path = label_path\n",
    "        self.filename_id_map = filename_id_map\n",
    "        self.ann_map = ann_map\n",
    "        self.files = files\n",
    "        self.processor = processor\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = tifffile.imread(os.path.join(self.data_path, self.files[idx]))\n",
    "        label = tifffile.imread(os.path.join(self.label_path, self.files[idx]))\n",
    "        bboxes = filename2bbox(self.files[idx], self.filename_id_map, self.ann_map)\n",
    "        if self.transform:\n",
    "            img, label = self.transform(img, label)\n",
    "        inputs = self.processor(img, input_boxes=bboxes, return_tensors='pt')\n",
    "        # remove batch dimension which the processor adds by default\n",
    "        inputs = {k:v.squeeze(0) for k,v in inputs.items()}\n",
    "        inputs['labels'] = torch.tensor(label).unsqueeze(0)\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the processor and model\n",
    "sam_processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")\n",
    "sam_model = SamModel.from_pretrained(\"facebook/sam-vit-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets and dataloaders\n",
    "train_dataset = SidewalkDataset(train_path, train_label_path, train_filename_id_map, train_ann_map, train_files, sam_processor)\n",
    "val_dataset = SidewalkDataset(val_path, val_label_path, val_filename_id_map, val_ann_map, val_files, sam_processor)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show example of the dataset\n",
    "sample = next(iter(train_dataloader))\n",
    "for k, v in sample.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure we only compute gradients for mask decoder\n",
    "for name, params in sam_model.named_parameters():\n",
    "    if name.startswith(\"vision_encoder\") or name.startswith(\"prompt_encoder\"):\n",
    "        params.requires_grad = False\n",
    "\n",
    "# Optimizer and loss function\n",
    "optimizer = Adam(sam_model.parameters(), lr=1e-5)\n",
    "loss_fn = DiceLoss(sigmoid=True, squared_pred=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "sam_model.to(device).train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_losses = []\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        # Forward pass\n",
    "        outputs = sam_model(pixel_values=batch['pixel_values'].to(device),\n",
    "                            input_boxes=batch['input_boxes'].to(device),\n",
    "                            multimask_output=False)\n",
    "        # Compute loss\n",
    "        predicted_masks = outputs.pred_masks.squeeze(1)\n",
    "        ground_truth_masks = batch['labels'].float().to(device)\n",
    "        loss = loss_fn(predicted_masks, ground_truth_masks.unsqueeze(1))\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {statistics.mean(epoch_losses)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model state dict\n",
    "torch.save(sam_model.state_dict(), '/content/drive/MyDrive/sam_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "sam_model.eval()\n",
    "val_losses = []\n",
    "for batch in tqdm(val_dataloader):\n",
    "    with torch.no_grad():\n",
    "        outputs = sam_model(pixel_values=batch['pixel_values'].to(device),\n",
    "                            input_boxes=batch['input_boxes'].to(device),\n",
    "                            multimask_output=False)\n",
    "        predicted_masks = outputs.pred_masks.squeeze(1)\n",
    "        ground_truth_masks = batch['labels'].float().to(device)\n",
    "        loss = loss_fn(predicted_masks, ground_truth_masks.unsqueeze(1))\n",
    "        val_losses.append(loss.item())\n",
    "\n",
    "print(f'Validation Loss: {statistics.mean(val_losses)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "sample = next(iter(val_dataloader))\n",
    "with torch.no_grad():\n",
    "    outputs = sam_model(pixel_values=sample['pixel_values'].to(device),\n",
    "                        input_boxes=sample['input_boxes'].to(device),\n",
    "                        multimask_output=False)\n",
    "    predicted_masks = outputs.pred_masks.squeeze(1)\n",
    "    ground_truth_masks = sample['labels'].float().to(device)\n",
    "\n",
    "%matplotlib inline\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "ax[0].imshow(sample['pixel_values'][0].cpu().numpy())\n",
    "ax[0].set_title('Image')\n",
    "ax[1].imshow(predicted_masks[0].cpu().numpy(), cmap='gray')\n",
    "ax[1].set_title('Predicted Mask')\n",
    "ax[2].imshow(ground_truth_masks[0].cpu().numpy(), cmap='gray')\n",
    "ax[2].set_title('Ground Truth Mask')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
